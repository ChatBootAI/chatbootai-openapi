openapi: 3.0.3
info:
  title: ChatBootAI
  version: 0.0.1
  description: >-
    This is ChatBootAI specification described as OpenAPI 3.0.


    ChatBoot AI allows you to interact with several user interface and back-ends
    following the same OpenAPI contract.
  license:
    name: Apache 2.0
    url: http://www.apache.org/licenses/LICENSE-2.0.html
externalDocs:
  url: https://github.com/ChatBootAI
  description: ChatBootAI GitHub repository
tags:
  - name: completions
    description: >-
      Given a list of messages comprising a conversation, the model returns a
      response
    externalDocs:
      url: https://platform.openai.com/docs/api-reference/chat
      description: OpenAI Chat API References
  - name: embeddings
    description: >-
      Vector representation of a given input that can be easily consumed by
      machine learning models and algorithms
    externalDocs:
      url: https://platform.openai.com/docs/api-reference/embeddings
      description: OpenAI Embeddings API References
  - name: moderations
    description: Classifies an input as violating moderation's content policy
    externalDocs:
      url: https://platform.openai.com/docs/api-reference/moderations
      description: OpenAI Moderations API References
paths:

    # ########### #
    #   OPEN AI   #
    # ########### #

  /chat/completions:
    summary: Engage a conversation
    description: >-
      Given a list of messages comprising a conversation, the model will return
      a response.
    post:
      tags:
        - completions
      summary: ''
      description: ''
      operationId: 'completion'
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/chatCompletionRequest"
      responses:
        '201':
          description: Default error sample response
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/chatCompletion"
              examples:
                Response for completion:
                  value: |
                    {
                      "id": "chatcmpl-123",
                      "object": "chat.completion",
                      "created": 1677652288,
                      "model": "gpt-3.5-turbo-0613",
                      "system_fingerprint": "fp_44709d6fcb",
                      "choices": [{
                        "index": 0,
                        "message": {
                          "role": "assistant",
                          "content": "\n\nHello there, how may I assist you today?",
                        },
                        "finish_reason": "stop"
                      }],
                      "usage": {
                        "prompt_tokens": 9,
                        "completion_tokens": 12,
                        "total_tokens": 21
                      }
                    }
                Response for streaming:
                  value: >
                    {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-3.5-turbo-0613",
                    "system_fingerprint": "fp_44709d6fcb",
                    "choices":[{"index":0,"delta":{"role":"assistant","content":""},"finish_reason":null}]}


                    {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-3.5-turbo-0613",
                    "system_fingerprint": "fp_44709d6fcb",
                    "choices":[{"index":0,"delta":{"content":"Hello"},"finish_reason":null}]}


                    {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-3.5-turbo-0613",
                    "system_fingerprint": "fp_44709d6fcb",
                    "choices":[{"index":0,"delta":{"content":"!"},"finish_reason":null}]}
  /embeddings:
    description: >-
      Get a vector representation of a given input that can be easily consumed
      by machine learning models and algorithms
    post:
      tags:
        - embeddings
      summary: ''
      description: ''
      operationId: 'embedding'
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/embeddingRequest"
      responses:
        '201':
          description: Default error sample response
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/embedding"
              examples:
                Response for embeddings:
                  value: |
                    {
                      "object": "list",
                      "data": [
                        {
                          "object": "embedding",
                          "embedding": [
                            0.0023064255,
                            -0.009327292,
                            .... (1536 floats total for ada-002)
                              -0.0028842222,
                          ],
                          "index": 0
                        }
                      ],
                      "model": "text-embedding-ada-002",
                      "usage": {
                        "prompt_tokens": 8,
                        "total_tokens": 8
                      }
                    }
  /moderations:
    description: >-
      Given a input text, outputs if the model classifies it as violating
      OpenAI's content policy.
    post:
      tags:
        - moderations
      summary: >-
        Given a input text, outputs if the model classifies it as violating
        OpenAI's content policy.
      description: ''
      operationId: 'moderation'
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/moderationRequest"
      responses:
        '201':
          description: Default error sample response
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/moderation"
              examples:
                Response for moderation:
                  value: |
                    {
                      "id": "modr-XXXXX",
                      "model": "text-moderation-005",
                      "results": [
                        {
                          "flagged": true,
                          "categories": {
                            "sexual": false,
                            "hate": false,
                            "harassment": false,
                            "self-harm": false,
                            "sexual/minors": false,
                            "hate/threatening": false,
                            "violence/graphic": false,
                            "self-harm/intent": false,
                            "self-harm/instructions": false,
                            "harassment/threatening": true,
                            "violence": true,
                          },
                          "category_scores": {
                            "sexual": 1.2282071e-06,
                            "hate": 0.010696256,
                            "harassment": 0.29842457,
                            "self-harm": 1.5236925e-08,
                            "sexual/minors": 5.7246268e-08,
                            "hate/threatening": 0.0060676364,
                            "violence/graphic": 4.435014e-06,
                            "self-harm/intent": 8.098441e-10,
                            "self-harm/instructions": 2.8498655e-11,
                            "harassment/threatening": 0.63055265,
                            "violence": 0.99011886,
                          }
                        }
                      ]
                    }
components:
  schemas:

    # ########### #
    #   OPEN AI   #
    # ########### #

    chatCompletionRequest:
      type: object
      properties:
        messages:
          type: array
          description: A list of messages comprising the conversation so far.
          items:
            type: object
            properties:
              content:
                type: string
                description: The contents of the system message.
              role:
                type: string
                description: The role of the messages author.
                enum:
                  - system
                  - user
                  - assistant
                  - tool
              name:
                type: string
                description: An optional name for the participant. Provides the model information to differentiate between participants of the same role.
            required:
              - content
              - role
            additionalProperties: false
        model:
          type: string
          description: ID of the model to use.
        frequency_penalty:
          type: number
          description: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
          default: 0
        max_tokens:
          type: number
          description: The maximum number of tokens to generate in the chat completion.
          default: infinity
        n:
          type: number
          description: How many chat completion choices to generate for each input message.
          default: 1
        presence_penalty:
          type: number
          description: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
          default: 0
        response_format:
          type: object
          description: An object specifying the format that the model must output.
          properties:
            type:
              type: string
              description: Must be one of text or json_object.
              default: text
          required:
            - content
            - role
          additionalProperties: false
        stop:
          type: string
          description: Up to 4 sequences where the API will stop generating further tokens.
          default: null
        stream:
          type: boolean
          description: If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data DONE
        temperature:
          type: number
          description: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or top_p but not both.
          default: 1
        top_p:
          type: number
          description: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or temperature but not both.
          default: 1
        user:
          type: string
          description: A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
      required:
        - model

    chatCompletion:
      type: object
      description: Represents a chat completion response returned by model, based on the provided input.
      properties:
        id:
          type: string
          description: A unique identifier for the completion.
        choices:
          type: array
          description: A list of chat completion choices. Can be more than one if n is greater than 1.
          items:
            type: object
            properties:
              finish_reason:
                type: string
                description: The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call (deprecated) if the model called a function.
                enum: [ "stop", "length", "content_filter" ]
              index:
                type: integer
                description: The index of the choice in the list of choices.
              message:
                $ref: "#/components/schemas/chatCompletionMessage"
            required:
              - finish_reason
              - index
              - text
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the completion was created.
        model:
          type: string
          description: The model used for the chat completion.
        system_fingerprint:
          type: string
          description: |
            This fingerprint represents the backend configuration that the model runs with.

            Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
        object:
          type: string
          description: The object type, which is always "chat.completion"
          enum: [ chat.completion ]
        usage:
          $ref: "#/components/schemas/chatCompletionUsage"
      required:
        - id
        - object
        - created
        - model
        - choices

    chatCompletionMessage:
      type: object
      description: A chat completion message generated by the model.
      properties:
        content:
          type: string
          description: The contents of the message.
          nullable: true
        tool_calls:
          $ref: "#/components/schemas/chatCompletionMessageToolCalls"
        role:
          type: string
          enum: [ "assistant" ]
          description: The role of the author of this message.
      required:
        - role
        - content

    chatCompletionMessageToolCalls:
      type: array
      description: The tool calls generated by the model, such as function calls.
      items:
        $ref: "#/components/schemas/chatCompletionMessageToolCall"

    chatCompletionMessageToolCall:
      type: object
      properties:
        id:
          type: string
          description: The ID of the tool call.
        type:
          type: string
          enum: [ "function" ]
          description: The type of the tool. Currently, only `function` is supported.
        function:
          type: object
          description: The function that the model called.
          properties:
            name:
              type: string
              description: The name of the function to call.
            arguments:
              type: string
              description: The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
          required:
            - name
            - arguments
      required:
        - id
        - type
        - function

    chatCompletionUsage:
      type: object
      description: Usage statistics for the completion request.
      properties:
        completion_tokens:
          type: integer
          description: Number of tokens in the generated completion.
        prompt_tokens:
          type: integer
          description: Number of tokens in the prompt.
        total_tokens:
          type: integer
          description: Total number of tokens used in the request (prompt + completion).
      required:
        - prompt_tokens
        - completion_tokens
        - total_tokens


    embeddingRequest:
      type: object
    embedding:
      type: object
    moderationRequest:
      type: object
    moderation:
      type: object

